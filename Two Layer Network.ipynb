{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXmQGNBWwzS1"
   },
   "source": [
    "### Introduction\n",
    "- In this assignment, you will build a two layer neural network for classification from scratch using only numpy.\n",
    "- Please refer to videos on Backpropagation and one reference material shared in additional resources for the understanding required to solve this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCZeHgK-xAy8"
   },
   "outputs": [],
   "source": [
    "\"\"\" Some functions required for testing \"\"\"\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model(D, H, C):\n",
    "  il = keras.layers.Input(shape=(D,))\n",
    "  hl = keras.layers.Dense(H, activation = 'relu')(il)\n",
    "  ol = keras.layers.Dense(C, activation = 'softmax')(hl)\n",
    "  model = keras.models.Model(inputs = [il], outputs = [ol])\n",
    "\n",
    "  rng = np.random.RandomState(2020)\n",
    "  model.layers[1].set_weights([rng.rand(D * H).reshape(D, H), rng.rand(H, )])\n",
    "  model.layers[2].set_weights([rng.rand(H * C).reshape(H, C), rng.rand(C, )])\n",
    "  return model\n",
    "\n",
    "def create_inputs(N, D):\n",
    "  rng = np.random.RandomState(2020)\n",
    "  return rng.rand(N * D).reshape(N, D)\n",
    "\n",
    "def set_weights_from_model(tln, test_net):\n",
    "  tln.params['W1'] = test_net.layers[1].get_weights()[0]\n",
    "  tln.params['b1'] = test_net.layers[1].get_weights()[1]\n",
    "  tln.params['W2'] = test_net.layers[2].get_weights()[0]\n",
    "  tln.params['b2'] = test_net.layers[2].get_weights()[1]\n",
    "  return tln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OoL62wOux9t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    D, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "\n",
    "    In other words, the network has the following architecture:\n",
    "\n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. \n",
    "        Weights are initialized to small random values and\n",
    "        biases are initialized to zero. \n",
    "        Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension N of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        D = input_size\n",
    "        H = hidden_size\n",
    "        C = output_size\n",
    "        self.params['W1'] = np.random.normal(0, std, D*H).reshape(D, H)\n",
    "        self.params['b1'] = np.zeros(H)\n",
    "        self.params['W2'] = np.random.normal(0, std, H*C).reshape(H, C)\n",
    "        self.params['b2'] = np.zeros(C)\n",
    "        ### Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ScjBCeTwzS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test Cases for Initialization\"\"\"\n",
    "tln = TwoLayerNet(2, 3, 2)\n",
    "assert tln.params['W1'].shape == (2, 3)\n",
    "assert tln.params['b1'].shape == (3, )\n",
    "assert tln.params['W2'].shape == (3, 2)\n",
    "assert tln.params['b2'].shape == (2, )\n",
    "print('Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh8LRCT9voz1"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(TwoLayerNet):\n",
    "\n",
    "    def forward(self, X):\n",
    "      \"\"\"\n",
    "      Compute the output of a full forward pass of the network.\n",
    "      \n",
    "      First apply weights W1 and biases b1 on inputs and then apply relu non-linearity.\n",
    "      Then apply weights W2 and biases b2 on hidden layer values and then apply softmax non-linearity to get the output\n",
    "      \n",
    "      Inputs:\n",
    "      - X : Input data of shape (N, D). Each X[i] is a training sample\n",
    "      \n",
    "      Outputs:\n",
    "      - y_out : numpy array with Outputs of shape (N, C)\n",
    "      \n",
    "      \"\"\"\n",
    "      ### Write your code here\n",
    "      z1 = X@self.params['W1'] + self.params['b1']\n",
    "      a1 = np.maximum(z1, 0)\n",
    "      z2 = a1@self.params['W2'] + self.params['b2']\n",
    "      a2_num = np.e**(z2)\n",
    "      a2_denom = np.sum(a2_num, axis = 1)\n",
    "      a2 = np.zeros(a2_num.shape)\n",
    "      for i in range(z2.shape[0]):\n",
    "        a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
    "      y_out = a2\n",
    "\n",
    "      return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2632,
     "status": "ok",
     "timestamp": 1589643033643,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "rJeXhkRZwzTD",
    "outputId": "d376c5c1-1020-4691-a710-0bc79d04febc"
   },
   "outputs": [],
   "source": [
    "\"\"\"Test Cases for Forward pass\"\"\"\n",
    "tln = TwoLayerNet(2, 4, 2)\n",
    "test_net = create_model(2, 4, 2)\n",
    "tln = set_weights_from_model(tln, test_net)\n",
    "X = create_inputs(4, 2)\n",
    "y_forward = tln.forward(X)\n",
    "assert y_forward.shape == (4, 2)\n",
    "assert np.all(np.isclose(y_forward, test_net.predict(X), atol = 0.0001))\n",
    "print('Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbOelhQCwzTH"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(TwoLayerNet):\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "          classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "          to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        y_out = self.forward(X)\n",
    "        y_pred = np.argmax(y_out, axis = 1)\n",
    "\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Implement this function; it should be VERY simple!                #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        ### Write your code here\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1898,
     "status": "ok",
     "timestamp": 1589643111506,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "AAJuwv5bwzTK",
    "outputId": "e18aec4c-0708-42fd-b2ee-9b10bd42c49a"
   },
   "outputs": [],
   "source": [
    "\"\"\" Test Cases for predict\"\"\"\n",
    "tln = TwoLayerNet(2, 4, 2)\n",
    "test_net = create_model(2, 4, 2)\n",
    "tln = set_weights_from_model(tln, test_net)\n",
    "X = create_inputs(4, 2)\n",
    "y_pred = tln.predict(X)\n",
    "test_pred = np.argmax(test_net.predict(X), axis = 1)\n",
    "assert np.all(np.isclose(y_pred, test_pred, atol = 0.01))\n",
    "print('Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tabhV02TwzTO"
   },
   "source": [
    "#### Loss\n",
    "Note: <br>\n",
    "$L = -\\sum{t_i \\log{p_i}}$ <br>\n",
    "where $p_i$ is probability score predicted by model. <br>\n",
    "$t_i = 1$ for the true class $i$ and $t_i = 0$ for all other classes for a particular sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mO3-DSCwzTO"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(TwoLayerNet):    \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples. (This is the mean loss over N samples)\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        H, C = W2.shape\n",
    "        \n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "        # Store the result in the scores variable, which should be an array of      #\n",
    "        # shape (N, C).                                                             #\n",
    "        #############################################################################\n",
    "        \n",
    "      \n",
    "        # Compute the forward pass\n",
    "        \n",
    "        z1 = X@self.params['W1'] + self.params['b1']\n",
    "        a1 = np.maximum(z1, 0)\n",
    "        z2 = a1@self.params['W2'] + self.params['b2']\n",
    "        a2_num = np.e**(z2)\n",
    "        a2_denom = np.sum(a2_num, axis = 1)\n",
    "        a2 = np.zeros(a2_num.shape)\n",
    "        for i in range(z2.shape[0]):\n",
    "          a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
    "        y_out = a2\n",
    "        scores = y_out\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Write your code here\n",
    "        \n",
    "        \n",
    "        \n",
    "        # # Compute the loss\n",
    "        loss = None\n",
    "        loss_samples = np.zeros(N)\n",
    "        for i in range(N):\n",
    "          loss_samples[i] = -np.log(scores[i, y[i]])\n",
    "        loss = np.mean(loss_samples)\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "        # in the variable loss, which should be a scalar. Use the Categorical       #\n",
    "        # Cross Entropy loss.                                                       #\n",
    "        #############################################################################\n",
    "      \n",
    "        ### Write your code here\n",
    "        \n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "\n",
    "        da2 = np.zeros((N, C))\n",
    "        for i in range(N):\n",
    "          da2[i, y[i]] = -1/a2[i, y[i]]\n",
    "        \n",
    "\n",
    "        da2_num = np.zeros((N, C))\n",
    "        da2_denom = np.zeros(N)\n",
    "        for i in range(N):\n",
    "          da2_num[i] = (1/a2_denom[i])*da2[i,: ]\n",
    "          da2_denom[i] = np.sum((-a2_num[i, :]/a2_denom[i]**2)*da2[i, :])\n",
    "\n",
    "        for i in range(N):\n",
    "          da2_num[i] += da2_denom[i]\n",
    "        \n",
    "        dz2 = a2_num * da2_num\n",
    "        da1 = dz2@self.params['W2'].T\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis = 0)\n",
    "        \n",
    "        \n",
    "        drelu = np.vectorize(lambda x: 1 if x>0 else 0)\n",
    "        dz1 = drelu(a1) * da1\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "        # and biases. Store the results in the grads dictionary. For example,       #\n",
    "        # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "        #############################################################################\n",
    "        ## We have to divide by N because aggregate loss = 1/N(Summation(-ti log yi))\n",
    "        grads['W1'] = dW1/N\n",
    "        grads['b1'] = db1/N\n",
    "        grads['W2'] = dW2/N\n",
    "        grads['b2'] = db2/N\n",
    "\n",
    "        ### Write your code here\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1589649860292,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "zn1tAYrpwzTS",
    "outputId": "694f192d-fe8c-4935-c408-80516c44cd5b"
   },
   "outputs": [],
   "source": [
    "\"\"\" Tests for loss and gradient computation \"\"\"\n",
    "### First compute loss and gradients using keras\n",
    "model = create_model(2, 4, 2)\n",
    "X = create_inputs(4, 2)\n",
    "y = np.array([0, 1, 1, 0])\n",
    "y_onehot = keras.utils.to_categorical(y, 2)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.0, nesterov=False, name=\"SGD\")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "batch_size = 4\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_out = model(x_batch_train, training = True)\n",
    "\n",
    "      ## Compute loss value for this minibatch\n",
    "      loss_value = loss_fn(y_batch_train, y_out)\n",
    "    \n",
    "    grads_model = {}\n",
    "    grads_model['W1'], grads_model['b1'], grads_model['W2'], grads_model['b2'] = [dw.numpy() for dw in tape.gradient(loss_value, model.trainable_weights)]\n",
    "\n",
    "### Compute loss and gradients using TwoLayerNet\n",
    "tln = TwoLayerNet(2, 4, 2)\n",
    "tln = set_weights_from_model(tln, model)\n",
    "loss, grads_tln = tln.loss(X, y)\n",
    "\n",
    "#### Now match\n",
    "## Loss should be correctly computed\n",
    "assert np.isclose(loss, loss_value.numpy(), atol = 0.0001)\n",
    "\n",
    "## Gradients should be correctly computed\n",
    "print(grads_tln['W1'], grads_model['W1'])\n",
    "assert np.all(np.isclose(grads_tln['W1'], grads_model['W1'], atol = 0.0001))\n",
    "assert np.all(np.isclose(grads_tln['b1'], grads_model['b1'], atol = 0.0001))\n",
    "assert np.all(np.isclose(grads_tln['W2'], grads_model['W2'], atol = 0.0001))\n",
    "assert np.all(np.isclose(grads_tln['b2'], grads_model['b2'], atol = 0.0001))\n",
    "\n",
    "print('Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANJB4KJni0ME"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyIYI5btwzTV"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(TwoLayerNet):\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "          X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        ## Create a copy of X and shuffle it\n",
    "        shuffled_indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        dataset_size = X.shape[0]\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for it in range(num_iters):\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "            # them in X_batch and y_batch respectively.                             #\n",
    "            #########################################################################\n",
    "\n",
    "            \n",
    "            ### Write your code here\n",
    "\n",
    "            start = (num_iters * batch_size)%dataset_size\n",
    "            X_batch = X_shuffled[start: start + batch_size]\n",
    "            y_batch = y_shuffled[start: start + batch_size]\n",
    "\n",
    "            \n",
    "            \n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "            # parameters of the network (stored in the dictionary self.params)      #\n",
    "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "            # stored in the grads dictionary defined above.                         #\n",
    "            #########################################################################\n",
    "\n",
    "            \n",
    "            ### Write your code here\n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b1'] -= learning_rate * grads['b1']\n",
    "            self.params['b2'] -= learning_rate * grads['b2']\n",
    "               \n",
    "\n",
    "            # Every epoch, check train and val accuracy\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                # learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYEfUA1nmCju"
   },
   "source": [
    "### Using these networks on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MsACefMSmFX0"
   },
   "source": [
    "### XOR\n",
    "Use TwoLayerNet to train the XOR function discussed in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7409,
     "status": "ok",
     "timestamp": 1589650758134,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "MeUsLKYEwzTY",
    "outputId": "c176b8a9-e433-4552-fc53-dfd1b6754a87"
   },
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "xor_net = TwoLayerNet(2, 5, 2)\n",
    "h = xor_net.train(X_xor, y_xor, X_xor, y_xor, batch_size = 4, num_iters = 20000, learning_rate = 0.01)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(h['loss_history'])\n",
    "print(h['loss_history'][-10:])\n",
    "xor_net.predict(X_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1589650776594,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "aPdW_QssjHzO",
    "outputId": "ff20a361-17d8-4921-d526-f87924b0f072"
   },
   "outputs": [],
   "source": [
    "plt.plot(h['val_acc_history'])\n",
    "h['val_acc_history'][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3D4GjcImFbW"
   },
   "source": [
    "### Iris\n",
    "Use TwoLayerNet to train the iris dataset. Choose 120 samples randomly for training and the rest for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1589651228998,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "jSjLcJT1mX-9",
    "outputId": "1303274d-7d38-47e3-e14d-cebdb9c81b8e"
   },
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "random_indices = np.arange(150)\n",
    "np.random.shuffle(random_indices)\n",
    "X_iris = iris['data'][random_indices[:120]]\n",
    "y_iris = iris['target'][random_indices[:120]]\n",
    "X_iris_val = iris['data'][random_indices[120:]]\n",
    "y_iris_val = iris['target'][random_indices[120:]]\n",
    "iris_net = TwoLayerNet(4, 64, 3)\n",
    "h = iris_net.train(X_iris, y_iris, X_iris_val, y_iris_val, num_iters = 1000, batch_size = 20, learning_rate = 0.05)\n",
    "plt.plot(h['loss_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1589651231537,
     "user": {
      "displayName": "Sai Saurab",
      "photoUrl": "",
      "userId": "06528725485567637895"
     },
     "user_tz": -330
    },
    "id": "B-eSD5PUl6hD",
    "outputId": "32977900-61de-4f00-ce33-47c63d8a9128"
   },
   "outputs": [],
   "source": [
    "plt.plot(h['val_acc_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbFD9EXlmaDI"
   },
   "source": [
    "### Advanced\n",
    "Add weight regularization to the loss and rewrite backprop part of TwoLayerNet. <br>\n",
    "Train using some datasets and see if regularized network performs better than its older counterpart.\n",
    "<br>\n",
    "The expression for loss with regularization is as follows - <br>\n",
    "$L = -\\sum{t_i \\log{p_i}} + \\lambda(|w_1|^2 + |w_2|^2)$ <br>\n",
    "$\\lambda$ is a tunable hyper-parameter  denoting strength of regularization. <br>\n",
    "If it is too high, network will struggle to fit, and if it is too low, network will overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBAJhyB9oB8M"
   },
   "outputs": [],
   "source": [
    "### Write your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Solutions of Assignment_9_Two_Layer_Neural_Network.ipynb",
   "provenance": [
    {
     "file_id": "1h7rKq4ZwB1RJ-_CRNdheW7gaWcJ4N9FM",
     "timestamp": 1589631141068
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
